# -*- coding: utf-8 -*-
"""ML ASSIGNMENT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K6J4Vvhs5xDbR4l2KuToL1RhB22htIG2

**1. Data Exploration and Preprocessing**
"""

import pandas as pd
import numpy as np

# Load data
from google.colab import files # Import the 'files' module from google.colab

print("Please upload the CSV file containing the dataset.")
uploaded = files.upload()

import io
# Load the dataset (assumes the uploaded file is an CSV file)
file_name = list(uploaded.keys())[0]
# Use io.BytesIO to create a file-like object from the bytes content
df = pd.read_csv(io.BytesIO(uploaded['TASK-ML-INTERN.csv']))

# Display basic information
print("\nDataset Overview:\n")
print(df.head())

"""Check for Missing Values"""

# Check for missing values
df.isnull().sum()

"""**Normalization/Standardization:**"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.drop(columns='hsi_id'))

"""**Visualization**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Average reflectance across all samples for each band
avg_reflectance = df.drop(columns='hsi_id').mean()
avg_reflectance.plot(kind='line', title='Average Spectral Reflectance')

"""** Dimensionality Reduction**"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_components = pca.fit_transform(df_scaled)

# Plotting the explained variance ratio
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')
plt.title('PCA Explained Variance')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.show()

"""**[  Model Training]**"""

#Neural Network Example:

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the model
model = Sequential()
model.add(Dense(64, input_dim=df_scaled.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # Output layer for regression

model.compile(optimizer='adam', loss='mean_squared_error')

# Train-Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_scaled, df['hsi_id'], test_size=0.2)

"""**Hyperparameter Tuning:**"""

from sklearn.model_selection import GridSearchCV
# Example for Grid Search with another regression model (e.g., RandomForest)
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor()
param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None]}
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')

"""**Model Evaluation**"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

y_pred = model.predict(X_test)

y_pred = model.predict(X_test)

mae = mean_absolute_error
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
print(f"MAE: {mae}, RMSE: {rmse}, RÂ²: {r2}")

"""**Visualization:**"""

plt.scatter(y_test, y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--k')
plt.xlabel('Actual DON Concentration')
plt.ylabel('Predicted DON Concentration')
plt.title('Actual vs Predicted DON Concentration')
plt.show()

